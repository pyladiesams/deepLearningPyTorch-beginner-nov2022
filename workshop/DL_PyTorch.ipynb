{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c5f00b",
   "metadata": {},
   "source": [
    "# Deep learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c00d73",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torchvision.datasets as datasets \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252c473",
   "metadata": {},
   "source": [
    "Tensors are data structure (similar to arrays and matrices, e.g. NumPy arrays).\n",
    "PyTorch uses tensors to encode the inputs and outputs of a model, as well as the modelâ€™s parameters.\n",
    "\n",
    "Ex 1.1:\n",
    "Create a tensor using torch class. Your tensor is a 2*3 matrix with values as 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee537e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor_a = \n",
    "tensor_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b9249",
   "metadata": {},
   "source": [
    "Ex 1.2: Then, create a tensor with random data and the previously defined dimensionality with torch.randn()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b6e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor_b =\n",
    "tensor_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14ae350",
   "metadata": {},
   "source": [
    "Ex 1.3:  Perform basic operation with pytorch: (1) concatenate the two tensors along the 1st dimension using torch.cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c8d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor_c = \n",
    "tensor_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf5c280",
   "metadata": {},
   "source": [
    "Ex. 2: Reshape tensor_c to 1 rows, 12 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9016a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor_r = \n",
    "tensor_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1adb82c",
   "metadata": {},
   "source": [
    "Now we need to load the data to feed our deep learning model. But first, we introduce a transform function that applies a transformation to a given input and outputs a new transformed version of the input. This is very useful for any data preprocessing and/or augmentation we need to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3516cbd6",
   "metadata": {},
   "source": [
    "Ex. 3: Can you describe the tranform function above? Which output do you expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec31f5",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a98cc",
   "metadata": {},
   "source": [
    "In many cases, we train neural networks on default or well-known datasets like MNIST. MNIST is a dataset consisting of handwritten images that are normalized and center-cropped. It has over 60,000 training images and 10,000 test images. To load and use the dataset we can apply the syntax below after the torchvision package is installed. \n",
    "PyTorch has other built-in datasets, which are pre-loaded in the class torch.datasets: let's have a look https://pytorch.org/vision/0.8/datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfe5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fde8ca",
   "metadata": {},
   "source": [
    "Now, it's time to load the imported dataset via the torch.utils.data.DataLoader class. It represents a Python iterable over a dataset, with support for iterable-style datasets, customizing data loading order, automatic batching, etc. \n",
    "\n",
    "Ex. 4. Use the torch.utils.data.DataLoader class to define a train and test loader with batch size equal to 10. Use: https://pytorch.org/docs/stable/data.html as guideline for learning the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47bf1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = \n",
    "#test_loader = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0528e4",
   "metadata": {},
   "source": [
    "##### Let's designe our neural network! \n",
    "\n",
    "The main ingredients are the input size, hidden layer size and output size with the activation function. To do so we will the nn.Module class for the neural network. We will start designing a simple feedforward network that has 3 layers: an input layer, a hidden layer, and an output layer.\n",
    "\n",
    "The self.linear1 is the input layer and takes in the parameters 28*28 because those are the amounts of pixels in each image, the output schould be 100.\n",
    "\n",
    "Ex. 5: Create the self.linear2 which is the hidden layer. It takes the output of the previous layer for the input, and has an output size of 50.\n",
    "Finally, we can design self.final which is the output layer. It takes the output of the previous layer for the input and will output size of 10 since we have 10 values within this dataset (0, 1, 2, 3, 4, 5, 6, 7, 8, 9).\n",
    "\n",
    "Lastly, we introduce our ReLU activation function, that will output the input if it is positive, otherwise output the value of zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655cfec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 100) \n",
    "        #self.linear2 = \n",
    "        #self.final = \n",
    "        self.relu = nn.ReLU()\n",
    "    # the forward function feeds the data through our network.\n",
    "    def forward(self, img):\n",
    "        x = img.view(-1, 28*28)  # reshape the images for the model.\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bad312c",
   "metadata": {},
   "source": [
    "We define the loss function as the cross-entropy, that measure of the difference between two probability distributions when given a random set of events (our dataset).\n",
    "\n",
    "We also define is our optimizer, which is Adam. \n",
    "\n",
    "Lastly, we have an epoch size of 10. The epoch is the training round through the full training set.\n",
    "A small number of epochs might prone to a poor learning, while a high number of epochs can lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69573bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_el = nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.Adam(net.parameters(), lr=0.001) #e-1\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c41b6b",
   "metadata": {},
   "source": [
    "We now iterate over the range of epochs and call the train() function to perform the training. \n",
    "x and y are the batch of features and targets, respectively\n",
    "\n",
    "Ex. 6 Given the loss above, calculate the loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d357cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(epoch):\n",
    "    running_loss=0\n",
    "    for data in train_loader:\n",
    "        x, y = data\n",
    "        optimizer.zero_grad() #set the gradient to zero before computing the loss\n",
    "        output = net(x.view(-1, 28*28)) #batch reshaping \n",
    "        #loss = \n",
    "        loss.backward() #loss back to the network parameters\n",
    "        optimizer.step() #optimize the weights taking into account the loss and the gradients\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(train_loader)}\")\n",
    "print('Finished training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38834a95",
   "metadata": {},
   "source": [
    "With t.no_grad() we do not perform gradients, and we can do he evaluation on our test set. \n",
    "We iterate over the test set and measure for accuracy of the model on unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28716d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "with t.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in test_loader:\n",
    "        x, y = data\n",
    "        output = net(x.view(-1, 784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if t.argmax(i) == y[idx]:\n",
    "                correct +=1\n",
    "            total +=1\n",
    "print(f'accuracy: {round(correct/total, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f99bc",
   "metadata": {},
   "source": [
    "Now it's time for some visual result interpretation. It's the number depicted in the picture the same given by the tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35331d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[6].view(28, 28))\n",
    "plt.show()\n",
    "print(t.argmax(net(x[6].view(-1, 784))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef27e59",
   "metadata": {},
   "source": [
    "Optional exercises:\n",
    "    \n",
    "Ex. 7 - Tune the learning rate, the optimizer, change the kernel sizes of the linear layers (Net). Do you find differences in the model learning and achieve accuracy? Can you discuss, argue, the results observed?\n",
    "\n",
    "Ex. 8 - Are you already a master of deep neaural networks? Can you replace our model with a 2D convolutional neural network? it is not necessary to re-design and re-train but can you give an insight on how would you proceed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9997b04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
